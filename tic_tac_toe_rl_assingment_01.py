# -*- coding: utf-8 -*-
"""Tic-Tac-Toe_RL_Assingment_01.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CnYqkdSckKg8eR9qG3q94jbL67L_DGBY
"""

import numpy as np
import random

# Define the Tic-Tac-Toe board
board = np.array([[' ' for _ in range(3)] for _ in range(3)])

# Define the Q-Learning agent's Q-table
Q = {}

# Define hyperparameters
learning_rate = 0.8
discount_factor = 1.0
exploration_prob = 0.2
epochs = 50000

# Function to check if a player has won
def check_win(player, board):
    for i in range(3):
        if all(board[i, :] == player) or all(board[:, i] == player):
            return True
    if all(np.diag(board) == player) or all(np.diag(np.fliplr(board)) == player):
        return True
    return False

# Function to get a list of available moves
def available_moves(board):
    return [(i, j) for i in range(3) for j in range(3) if board[i, j] == ' ']

# Function to choose a move for the Q-Learning agent
def choose_move(board):
    if random.uniform(0, 1) < exploration_prob:
        return random.choice(available_moves(board))
    else:
        max_q = -float('inf')
        best_move = None
        for move in available_moves(board):
            next_board = board.copy()
            next_board[move] = 'O'
            next_state = tuple(map(tuple, next_board))
            if next_state not in Q:
                Q[next_state] = 0
            if Q[next_state] > max_q:
                max_q = Q[next_state]
                best_move = move
        return best_move

# Function to train the Q-Learning agent
def train_agent():
    for _ in range(epochs):
        board = np.array([[' ' for _ in range(3)] for _ in range(3)])
        state_history = []

        while True:
            move = choose_move(board)
            state_history.append(tuple(map(tuple, board)))
            board[move] = 'O'

            if check_win('O', board):
                reward = 1
                break
            elif len(available_moves(board)) == 0:
                reward = 0
                break

            opponent_move = random.choice(available_moves(board))
            board[opponent_move] = 'X'

            if check_win('X', board):
                reward = -1
                break

        for state in state_history:
            if state not in Q:
                Q[state] = 0
            Q[state] += learning_rate * (reward - Q[state])

# Function to play the game
def play_game():
    board = np.array([[' ' for _ in range(3)] for _ in range(3)])
    while True:
        print(board)
        row = int(input("Enter the row (0, 1, 2): "))
        col = int(input("Enter the column (0, 1, 2): "))
        if (row, col) not in available_moves(board):
            print("Invalid move. Try again.")
            continue
        board[(row, col)] = 'X'

        if check_win('X', board):
            print(board)
            print("You win!")
            break

        move = choose_move(board)
        print(f"Agent chooses: {move}")
        board[move] = 'O'

        if check_win('O', board):
            print(board)
            print("Agent wins!")
            break

        if len(available_moves(board)) == 0:
            print(board)
            print("It's a draw!")
            break

if __name__ == "__main__":
    train_agent()
    play_game()